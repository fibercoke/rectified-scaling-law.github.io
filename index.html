<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="description" content="Selecting Large Language Model to Fine-tune via Rectified Scaling Law">
    <meta name="keywords" content="LLM, Scaling Law, Finetuning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Selecting Large Language Model to Fine-tune via Rectified Scaling Law</title>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="index.css">
    <!-- <link rel="icon" href="images/logo.png"> -->
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LCS0X09QWB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-LCS0X09QWB');
  </script>

  <style>
    .expandable-card .card-text-container {
      max-height: 200px;
      overflow-y: hidden;
      position: relative;
    }

    .expandable-card.expanded .card-text-container {
      max-height: none;
    }

    .expand-btn {
      position: relative;
      display: none;
      background-color: rgba(255, 255, 255, 0.8);
      /* margin-top: -20px; */
      /* justify-content: center; */
      color: #510c75;
      border-color: transparent;
    }

    .expand-btn:hover {
      background-color: rgba(200, 200, 200, 0.8);
      text-decoration: none;
      border-color: transparent;
      color: #510c75;
    }

    .expand-btn:focus {
      outline: none;
      text-decoration: none;
    }

    .expandable-card:not(.expanded) .card-text-container:after {
      content: "";
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 90px;
      background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
    }

    .expandable-card:not(.expanded) .expand-btn {
      margin-top: -40px;
    }

    .card-body {
      padding-bottom: 5px;
    }

    .vertical-flex-layout {
      justify-content: center;
      align-items: center;
      height: 100%;
      display: flex;
      flex-direction: column;
      gap: 5px;
    }

    .figure-img {
      max-width: 100%;
      height: auto;
    }

    .adjustable-font-size {
      font-size: calc(0.5rem + 2vw);
    }

    .chat-history {
      flex-grow: 1;
      overflow-y: auto;
      /* overflow-x: hidden; */
      padding: 5px;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }

    #gradio pre {
      background-color: transparent;
    }
  </style>

  <body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <!-- <h1 class="title is-1 publication-title"><img id="logo" width="10%" src="./static/images/logo.png"> RAT</h1> -->
              <!-- <h1 class="title is-2 publication-title">Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</h1> -->
              <h1 class="title is-1 publication-title">Selecting Large Language Model to Fine-tune via Rectified Scaling Law</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://linhaowei1.github.io/" target="_blank">Haowei Lin*</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="http://craftjarvis.org/rectified-scaling-law/" target="_blank">Baizhou Huang*</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://haotianye.com/" target="_blank">Haotian Ye*</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://qinyuchen.site" target="_blank">Qinyu Chen</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="http://zhwang4ai.github.io/" target="_blank">Zihao Wang</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://pku-tangent.github.io/" target="_blank">Sujian Li</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://majianzhu.com/" target="_blank">Jianzhu Ma</a><sup>3</sup>,
                </span>
                <span class="author-block">
                  <a href="https://wanxiaojun.github.io/" target="_blank">Xiaojun Wan</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://www.james-zou.com/" target="_blank">James Zou</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=KVzR1XEAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yitao Liang</a><sup>1</sup>
                </span>
              </div>
    
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <sup>1</sup>Peking University,
                </span>
                <span class="author-block"><sup>2</sup>Stanford University,</span>
                <span class="author-block"><sup>3</sup>Tsinghua University</span>
              </div>

              <div class="is-size-4 publication-authors">
                <span class="conference-block">
                  ICML 2024
                </span>
              </div>
    
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.02314" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2402.02314" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a href="https://github.com/linhaowei1/Fine-tuning-Scaling-Law" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                      </a>
                  </span>
                  <!-- Demo Link. -->
                  <!-- <span class="link-block">
                    <a href="https://huggingface.co/spaces/jeasinema/RAT" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="far fa-images"></i>
                      </span>
                      <span>Demo</span>
                      </a>
                  </span> -->
                </div>
    
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The ever-growing ecosystem of LLMs has posed a challenge in selecting the most appropriate pre-trained model to fine-tune amidst a sea of options. Given constrained resources, fine-tuning all models and making selections afterward is unrealistic. In this work, we formulate this resource-constrained selection task into predicting fine-tuning performance and illustrate its natural connection with Scaling Law. Unlike pre-training, we find that the fine-tuning scaling curve includes not just the well-known "power phase" but also the previously unobserved "pre-power phase". We also explain why existing Scaling Law fails to capture this phase transition phenomenon both theoretically and empirically. To address this, we introduce the concept of "pre-learned data size" into our Rectified Scaling Law, which overcomes theoretical limitations and fits experimental results much better. By leveraging our law, we propose a novel LLM selection algorithm that selects the near-optimal model with hundreds of times less resource consumption, while other methods may provide negatively correlated selection.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      </div>
    </section>

    <section class="section" id="tldr" style="background-color:#efeff081">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">TL;DR</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <font color="#224baf" size=4><b> -->
            <div class="content has-text-justified">
              <p>
                We first formulated the problem of LLM selection and rectified the Scaling Law for fine-tuning senarios. Based on this rectified scaling law, we designed an effective method to address model selection problem.
              </p>
            </div>
            <!-- </b></font> -->
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="Figure1" >
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Scaling Law Fails?</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <font color="#224baf" size=4><b> -->
            <div class="content has-text-justified">
              <p>
                <!-- We delineate the process of utilizing the seed captions to train a general captioner (Share-Captioner)
                and then employing this captioner to generate <b>1.2M high-quality captions</b> for pre-training usage. -->
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="./static/img/intro.png">     
                  </div>
                </centering>
                <li> Figure a demonstrates the Pearson correlation between the true full-fine-tuning performance and the predicted performance of 3 heuristic methods, given different resource constraints denoted by &gamma;.
                These baseline methods cannot predict performance well especially under demanding constraints (small \( &gamma; \)), and could even provide negatively correlated predictions. 
                <li> Figure b displays the phase transition phenomenon observed in the scaling of fine-tuning loss \(L\) with training sample size \(D\). 
                 In addition to the widely studied power phase where \((L,D)\) are linearly correlated under the log-log scale, we discover the pre-power phase when \(D\) is small. Previous laws fail to fit both phases, while our proposed law fits quite well.
                <li> Figure c summarizes our LLM selection algorithm that extrapolates full-fine-tuning performance based on the new law. 
              </p>
            </div>
            <!-- </b></font> -->
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="Figure2" style="background-color:#efeff081">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <!-- <h2 class="title is-3">Pipeline</h2> -->
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <font color="#224baf" size=4><b> -->
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="90%" src="static/img/figure2.PNG">     
              </div>
            </centering>
            <p>
              We chose 3 datasets and plotted the phase transition from pre-power phase to power phase, and the fitness of different Scaling Laws. The x and y axes are fine-tuning dataset size D and test loss L in log scale. Each subfigure corresponds to a dataset. Solid lines are the fitting results of our law, and dash lines are the fitting results of vanilla law.
            </p>
            <br>
            <!-- <centering>
              <div style="text-align: center;">
                <img id="teaser" width="80%" src="static/images/algorithm.png">     
              </div>
            </centering> -->
            </b>
          <!-- </font>            -->
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="Algorithm" >
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Algorithm and Findings</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <font color="#224baf" size=4><b> -->
            <!-- <centering>
              <div style="text-align: center;">
                <img id="teaser" width="90%" src="static/img/figure2.PNG">     
              </div>
            </centering> -->
            <p>
              The vanilla scaling law is:
              <p>
                $$
                \mathcal{L}(D) = \left(\frac{B}{D^\beta}+E\right)^\alpha.
                $$
              </p>
              We define the rectified scaling law with dataset size \(D\) for  fine-tuning as:
              <p>
                $$
                \mathcal{L}(D) = \left(\frac{B}{D_l+D^\beta}+E\right)^\alpha.
                $$
              </p>
              The algorithm is as follows:
            </p>
            <br>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="50%" src="static/img/algorithm.png">     
              </div>
            </centering>
            </b>
          <!-- </font>            -->
          </div>
        </div>
      </div>
    </section>
    
    <section class="section" id="Results" style="background-color:#efeff081">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Results</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <!-- <font color="#224baf" size=4><b> -->
            <div class="content has-text-justified">
              <p>
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="./static/img/results.PNG">     
                  </div>
                </centering>
                <p></p>
                We compared the proposed model selection method (AtS) with 3 intuitive methods. Results show that our method excels under different training budget ratios.
                <p></p>
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="./static/img/model_info.png">     
                  </div>
                </centering>
                <p></p>
                This table summarizes all the models we used in experiments. The Arch. is short for model architecture, De-only, En-De and Moe stands for Decoder-only, Encoder-Decoder and Mixture of Experts respectively. The last few columns summarize the configuration of different language models, including number of parameters, number of layers, dimension of hidden states, number of attention heads, dimension of feed-forward layers, and dimension of key/value head.
                <p></p>
              </p>
            </div>
            <!-- </b></font> -->
          </div>
        </div>
      </div>
    </section>
    
    <!-- <section class="section" style="background-color:#efeff081" id="demo">
      <div class="container is-max-desktop" id="gradio">
        <h2 class="title">Gradio Apps</h2>
        <gradio-app src="https://huggingface.co/spaces/jeasinema/RAT"></gradio-app> 
      </div>
    </section> -->
    
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{lin2024selecting,
      author    = {Haowei, Lin and Baizhou, Huang and Haotian, Ye and Qinyu, Chen and Zihao, Wang and Sujian, Li and Jianzhu, Ma and Xiaojun, Wan and James, Zou and Yitao, Liang},
      title     = {Selecting Large Language Model to Fine-tune via Rectified Scaling Law},
      journal   = {ICML},
      year      = {2024},
    }</code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is adapted from <a target="_blank"
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, with <a rel="license" target="_blank"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  
  </body>

</html>
